{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GraSP pruning","metadata":{}},{"cell_type":"code","source":"!pip install torchbearer","metadata":{"execution":{"iopub.status.busy":"2023-05-16T16:49:21.535445Z","iopub.execute_input":"2023-05-16T16:49:21.535851Z","iopub.status.idle":"2023-05-16T16:49:33.619109Z","shell.execute_reply.started":"2023-05-16T16:49:21.535808Z","shell.execute_reply":"2023-05-16T16:49:33.617995Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchbearer\n  Downloading torchbearer-0.5.3-py3-none-any.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchbearer) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchbearer) (1.23.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchbearer) (4.64.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchbearer) (4.5.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchbearer) (3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchbearer) (3.11.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchbearer) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchbearer) (1.11.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchbearer) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchbearer) (1.3.0)\nInstalling collected packages: torchbearer\nSuccessfully installed torchbearer-0.5.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.nn.utils import prune\nimport torchvision.transforms as transforms\nimport torchbearer\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import vgg16_bn, vgg19_bn\nfrom torchbearer import Trial\nimport numpy as np\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-05-16T16:49:40.316724Z","iopub.execute_input":"2023-05-16T16:49:40.317792Z","iopub.status.idle":"2023-05-16T16:49:43.863795Z","shell.execute_reply.started":"2023-05-16T16:49:40.317754Z","shell.execute_reply":"2023-05-16T16:49:43.862613Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# fix random seed for reproducibility\nseed = 7\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-05-16T16:49:43.870030Z","iopub.execute_input":"2023-05-16T16:49:43.870652Z","iopub.status.idle":"2023-05-16T16:49:43.883833Z","shell.execute_reply.started":"2023-05-16T16:49:43.870616Z","shell.execute_reply":"2023-05-16T16:49:43.882727Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation\nThe CIFAR-10 dataset is downloaded and transformed with a batch size of 128, using the same parameters as the source research paper [1]. The training dataset is transformed by random crop followed by horizontal flips.","metadata":{}},{"cell_type":"code","source":"train_batch_size = 128\ntest_batch_size = 128\n\n# convert each image to tensor format\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n# load data\ntrainset = CIFAR10(root='.', train=True, download=True, transform=transform_train)\ntestset = CIFAR10(root='.', train=False, download=True, transform=transform_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T16:49:46.506117Z","iopub.execute_input":"2023-05-16T16:49:46.506656Z","iopub.status.idle":"2023-05-16T16:49:56.453644Z","shell.execute_reply.started":"2023-05-16T16:49:46.506615Z","shell.execute_reply":"2023-05-16T16:49:56.452739Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:05<00:00, 28599440.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./cifar-10-python.tar.gz to .\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# create data loaders\ntrainloader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\ntestloader = DataLoader(testset, batch_size=test_batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T16:49:56.455507Z","iopub.execute_input":"2023-05-16T16:49:56.455857Z","iopub.status.idle":"2023-05-16T16:49:56.460883Z","shell.execute_reply.started":"2023-05-16T16:49:56.455824Z","shell.execute_reply":"2023-05-16T16:49:56.460028Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model\nThe default VGG16 model from Pytorch is modified to align with the CIFAR-10 dataset which has 10 output classes.","metadata":{}},{"cell_type":"code","source":"num_classes = 10\n\nmodel = vgg16_bn()\nmodel.features = model.features[:-1]\nmodel.avgpool = nn.AvgPool2d(2)\nmodel.classifier = nn.Linear(512, num_classes)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:04:59.594972Z","iopub.execute_input":"2023-05-16T17:04:59.595332Z","iopub.status.idle":"2023-05-16T17:05:02.286707Z","shell.execute_reply.started":"2023-05-16T17:04:59.595301Z","shell.execute_reply":"2023-05-16T17:05:02.285758Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n  )\n  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (classifier): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Reinitialised weights using [He initialisation](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_) method for all linear and convolution layers. And using random weight from a uniform distribution for Batch normalization weight with zero bias.","metadata":{}},{"cell_type":"code","source":"def init_weights(m):\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight)\n    elif isinstance(m, torch.nn.BatchNorm2d):\n        m.weight.data = torch.rand(m.weight.data.shape)\n        m.bias.data = torch.zeros_like(m.bias.data)\n\nmodel = model.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:05:02.288728Z","iopub.execute_input":"2023-05-16T17:05:02.289092Z","iopub.status.idle":"2023-05-16T17:05:02.392048Z","shell.execute_reply.started":"2023-05-16T17:05:02.289058Z","shell.execute_reply":"2023-05-16T17:05:02.391121Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## GraSP Pruning","metadata":{}},{"cell_type":"code","source":"def data_sampling(trainset):\n    # Sample 10 data from each class (100 data in total)\n    s_inputs = []\n    s_targets = []\n    samples = next(iter(DataLoader(trainset, batch_size=300, shuffle=True)))\n    for t in set(trainset.targets):\n        indices = random.sample([i for i, x in enumerate(samples[1]) if x == t], 10)\n        s_inputs += [samples[0][i].tolist() for i in indices]\n        s_targets += [samples[1][i].tolist() for i in indices]\n\n    s_inputs = torch.Tensor(np.array(s_inputs))\n    s_targets = torch.Tensor(np.array(s_targets)).to(torch.long)\n\n    print(s_inputs.shape)\n    print(s_targets.shape)\n    return s_inputs, s_targets\n\ndef magnitude_score(model, trainset):\n    '''calculate pruning score for all prune-able layers\n        return: {layer: tensor of score for each weight in the layer}'''\n    # modified from: https://github.com/alecwangcq/GraSP/blob/master/pruner/GraSP.py\n    T = 200\n    model.zero_grad()\n    weights = [weight for name, weight in model.named_parameters() if name.endswith('.weight')]\n    \n    # sampling from trainset\n    s_inputs, s_targets = data_sampling(trainset)\n    \n    # compute the Hessian-gradient\n    outputs = model.forward(s_inputs)/T\n    loss = F.cross_entropy(outputs, s_targets)\n    grad_w = list(torch.autograd.grad(loss, weights))\n\n    outputs = model.forward(s_inputs)/T\n    loss = F.cross_entropy(outputs, s_targets)\n    grad_f = list(torch.autograd.grad(loss, weights, create_graph=True))\n\n    z = sum([(gw.data * gf).sum() for gw, gf in zip(grad_w, grad_f)])\n    z.backward()\n\n    scores = {}\n    for name, weight in model.named_parameters():\n        if name.endswith('.weight'):\n            # score is calculated by -weight * Hessian-gradient\n            scores[name.replace('.weight', '')] = -weight.detach() * weight.grad\n    return scores\n\ndef create_mask(model, scores, sparse_ratio, prune_type='min', random_shuffling=False):\n    # flatted all score to a vector \n    # modified from: https://github.com/alecwangcq/GraSP/blob/master/pruner/GraSP.py\n    score_vec = torch.cat([torch.flatten(x) for x in scores.values()])\n    \n    # nomalisation \n    eps = 1e-10\n    norm_factor = torch.abs(torch.sum(score_vec)) + eps\n    score_vec.div_(norm_factor)\n    \n    # calculate number of parameters to prune\n    num_prune = np.ceil(len(score_vec) * sparse_ratio).astype(int)\n    num_keep = (score_vec.shape - num_prune)[0]\n    print(\"Number of params to prune:\", num_prune)\n    print(\"Remaining params:\", num_keep)\n    \n    if prune_type == 'top':\n        # prune top k score\n        threshold = torch.topk(score_vec, num_prune, sorted=True)[0][-1]\n        print('threshold', threshold.data)\n    elif prune_type == 'min':\n        # prune min k score\n        threshold = torch.topk(score_vec, num_keep, sorted=True)[0][-1]\n        print('threshold', threshold.data)\n        \n    # create mask\n    masks = {}\n    named_modules = dict(model.named_modules())\n    \n    for m, g in scores.items():\n        layer = named_modules[m]\n        if prune_type == 'top':\n            # prune top k score\n            masks[layer] = ((g / norm_factor) <= threshold).float()\n        elif prune_type == 'min':\n            # prune min k score\n            masks[layer] = ((g / norm_factor) >= threshold).float()\n            \n        if random_shuffling:\n            ## randomly shuffle weight within each layer\n            idx = torch.randperm(masks[layer].nelement())\n            masks[layer] = masks[layer].view(-1)[idx].view(masks[layer].size())\n            \n    print('masks', torch.sum(torch.cat([torch.flatten(x == 1) for x in masks.values()])))\n    return masks\n\ndef prune_model(model, masks, reinit=False):\n    if reinit:\n        # re-initialise weight\n        model = model.apply(init_weights)\n\n    for m in masks.keys():\n        m = prune.custom_from_mask(m, name='weight', mask=masks[m].data)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:05:04.202652Z","iopub.execute_input":"2023-05-16T17:05:04.204009Z","iopub.status.idle":"2023-05-16T17:05:04.225293Z","shell.execute_reply.started":"2023-05-16T17:05:04.203962Z","shell.execute_reply":"2023-05-16T17:05:04.224321Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"sparse_ratio = 0.9\n# for inversion, use prune_type = 'min'\nprune_type = 'top' # ['top', 'min']\nrandom_shuffling = False\nreinit = False\n\nscores = magnitude_score(model, trainset)\nmasks = create_mask(model, scores, sparse_ratio, prune_type, random_shuffling)\nmodel = prune_model(model, masks, reinit)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:05:04.794290Z","iopub.execute_input":"2023-05-16T17:05:04.794914Z","iopub.status.idle":"2023-05-16T17:05:16.563880Z","shell.execute_reply.started":"2023-05-16T17:05:04.794870Z","shell.execute_reply":"2023-05-16T17:05:16.562848Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"torch.Size([100, 3, 32, 32])\ntorch.Size([100])\nNumber of params to prune: 13247828\nRemaining params: 1471980\nthreshold tensor(-1.0171e-05)\nmasks tensor(1471981)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss(train_loss, test_loss):\n    plt.plot(train_loss, label=\"Training data\")\n    plt.plot(test_loss, label=\"Validation data\")\n    plt.xlabel(\"Epochs\", fontsize=\"18\")\n    plt.ylabel(\"Loss\", fontsize=\"18\")\n    plt.tick_params(axis='both', which='major', labelsize=15)\n    plt.legend(fontsize=\"15\")\n    plt.grid()\n    plt.show();\n\ndef plot_acc(train_acc, test_acc):\n    plt.plot(train_acc, label=\"Training data\")\n    plt.plot(test_acc, label=\"Validation data\")\n    plt.xlabel(\"Epochs\", fontsize=\"18\")\n    plt.ylabel(\"Accuracy\", fontsize=\"18\")\n    plt.tick_params(axis='both', which='major', labelsize=15)\n    plt.legend(fontsize=\"15\")\n    plt.grid();","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:06:00.872248Z","iopub.execute_input":"2023-05-16T17:06:00.872597Z","iopub.status.idle":"2023-05-16T17:06:00.880182Z","shell.execute_reply.started":"2023-05-16T17:06:00.872570Z","shell.execute_reply":"2023-05-16T17:06:00.879232Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from torchbearer import Callback\nfrom torchbearer import callbacks\nfrom torchbearer.callbacks import MultiStepLR\n\n@callbacks.on_end_epoch\ndef callback(state):\n    try:\n        train_loss[state[torchbearer.state.EPOCH]] = state[torchbearer.state.METRICS]['loss']\n        train_acc[state[torchbearer.state.EPOCH]] = state[torchbearer.state.METRICS]['acc']\n        test_loss[state[torchbearer.state.EPOCH]] = state[torchbearer.state.METRICS]['val_loss']\n        test_acc[state[torchbearer.state.EPOCH]] = state[torchbearer.state.METRICS]['val_acc']\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:06:01.155584Z","iopub.execute_input":"2023-05-16T17:06:01.155962Z","iopub.status.idle":"2023-05-16T17:06:01.162365Z","shell.execute_reply.started":"2023-05-16T17:06:01.155930Z","shell.execute_reply":"2023-05-16T17:06:01.161491Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# train the model using cross entropy loss with loss ratio scheduler and SDG optimiser\ndef train_model(model, epochs=80):\n    model = model.to(device)\n    loss_function = nn.CrossEntropyLoss()\n    scheduler = callbacks.MultiStepLR(milestones=[40, 60], gamma=0.1)\n    optimiser = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n\n    trial = torchbearer.Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], callbacks=[callback, scheduler]).to(device)\n    trial.with_generators(trainloader, test_generator=testloader, val_generator=testloader)\n    trial.run(epochs)\n    results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n    print(results)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:06:01.615163Z","iopub.execute_input":"2023-05-16T17:06:01.615515Z","iopub.status.idle":"2023-05-16T17:06:01.625875Z","shell.execute_reply.started":"2023-05-16T17:06:01.615487Z","shell.execute_reply":"2023-05-16T17:06:01.622067Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"num_epochs = 80\n\n# save loss and accuracy during training\ntrain_loss = np.zeros(num_epochs)\ntrain_acc = np.zeros(num_epochs)\ntest_loss = np.zeros(num_epochs)\ntest_acc = np.zeros(num_epochs)\n\n# train model\ntrain_model(model, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T17:06:03.162844Z","iopub.execute_input":"2023-05-16T17:06:03.163311Z","iopub.status.idle":"2023-05-16T17:07:40.297126Z","shell.execute_reply.started":"2023-05-16T17:06:03.163274Z","shell.execute_reply":"2023-05-16T17:07:40.296182Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"0/2(t):   0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0f067d2f3434b07a064aeb98cb1f095"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0/2(v):   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9348230100064355b30c5463d33aadf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1/2(t):   0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"940c5021e7d34f2884ad8cbbd5b4e420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1/2(v):   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e554c2226d4bd6bca09a839653deec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0/1(e):   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bda8d9aa8f44d2ac5f6c3ac2815bcd"}},"metadata":{}},{"name":"stdout","text":"{'test_loss': 1.0348103046417236, 'test_acc': 0.6349999904632568}\n","output_type":"stream"}]},{"cell_type":"code","source":"plot_loss(train_loss, test_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc(train_acc, test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"./weight.weights\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n[1] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural net- works at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}